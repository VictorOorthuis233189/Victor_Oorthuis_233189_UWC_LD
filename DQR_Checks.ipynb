{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f804ef",
   "metadata": {},
   "source": [
    "### Check 1: Validity\n",
    "\n",
    "To check whether data is valid, we will check if the landmark coordinates are within the image. For example, a coordinate can not be 3000, 3000 if the image resolution is 1800x2400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2643d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe2013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking: cks2ip8fq29yq0yufc4scftj8.json\n",
      "With image: cks2ip8fq29yq0yufc4scftj8.png\n",
      "\n",
      "Image resolution: 1968 x 2225\n",
      "\n",
      "Checking 29 landmarks:\n",
      "----------------------------------------\n",
      "✓ A-point: (1315, 1086)\n",
      "✓ Anterior Nasal Spine: (1338, 1048)\n",
      "✓ B-point: (1333, 1564)\n",
      "✓ Menton: (1297, 1733)\n",
      "✓ Nasion: (1183, 508)\n",
      "✓ Orbitale: (1112, 790)\n",
      "✓ Pogonion: (1348, 1663)\n",
      "✓ Posterior Nasal Spine: (793, 1138)\n",
      "✓ Pronasale: (1585, 946)\n",
      "✓ Ramus: (523, 1265)\n",
      "✓ Sella: (499, 758)\n",
      "✓ Articulare: (445, 1061)\n",
      "✓ Condylion: (449, 980)\n",
      "✓ Gnathion: (1336, 1707)\n",
      "✓ Gonion: (593, 1496)\n",
      "✓ Porion: (291, 958)\n",
      "✓ Lower 2nd PM Cusp Tip: (1189, 1329)\n",
      "✓ Lower Incisor Tip: (1371, 1308)\n",
      "✓ Lower Molar Cusp Tip: (1147, 1334)\n",
      "✓ Upper 2nd PM Cusp Tip: (1177, 1339)\n",
      "✓ Upper Incisor Apex: (1288, 1106)\n",
      "✓ Upper Incisor Tip: (1400, 1334)\n",
      "✓ Upper Molar Cusp Tip: (1119, 1339)\n",
      "✓ Lower Incisor Apex: (1288, 1556)\n",
      "✓ Labrale inferius: (1501, 1369)\n",
      "✓ Labrale superius: (1508, 1212)\n",
      "✓ Soft Tissue Nasion: (1240, 557)\n",
      "✓ Soft Tissue Pogonion: (1488, 1608)\n",
      "✓ Subnasale: (1458, 1067)\n",
      "----------------------------------------\n",
      "\n",
      "Result: 0 invalid landmarks out of 29\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "annotations_dir = Path(r\"C:\\Users\\victo\\Downloads\\Aariz\\Aariz\\train\\Annotations\\Cephalometric Landmarks\\Senior Orthodontists\")\n",
    "images_dir = Path(r\"C:\\Users\\victo\\Downloads\\Aariz\\Aariz\\train\\Cephalograms\")\n",
    "\n",
    "# %%\n",
    "# Get first annotation and image\n",
    "annotation_file = list(annotations_dir.glob(\"*.json\"))[0]\n",
    "image_file = list(images_dir.glob(\"*.*\"))[0]  # Adjust extension if needed\n",
    "\n",
    "print(f\"Checking: {annotation_file.name}\")\n",
    "print(f\"With image: {image_file.name}\")\n",
    "\n",
    "# %%\n",
    "# Get image resolution\n",
    "img = Image.open(image_file)\n",
    "width, height = img.size\n",
    "print(f\"\\nImage resolution: {width} x {height}\")\n",
    "\n",
    "# %%\n",
    "# Load annotation and check each landmark\n",
    "with open(annotation_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"\\nChecking {len(data['landmarks'])} landmarks:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "invalid_count = 0\n",
    "for landmark in data['landmarks']:\n",
    "    name = landmark['title']\n",
    "    x = landmark['value']['x']\n",
    "    y = landmark['value']['y']\n",
    "    \n",
    "    # Check if within bounds (0 <= x < width, 0 <= y < height)\n",
    "    if not (0 <= x < width and 0 <= y < height):\n",
    "        print(f\"{name}: ({x}, {y}) - OUT OF BOUNDS\")\n",
    "        invalid_count += 1\n",
    "    else:\n",
    "        print(f\"✓ {name}: ({x}, {y})\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"\\nResult: {invalid_count} invalid landmarks out of {len(data['landmarks'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a91cb937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for duplicate images...\n",
      "----------------------------------------\n",
      "Found 700 image files\n",
      "Computing hashes...\n",
      "\n",
      "========================================\n",
      "RESULTS\n",
      "========================================\n",
      "⚠️  Found 11 groups of duplicate images:\n",
      "\n",
      "Duplicate Group 1 (2 identical files):\n",
      "  - cl5lg05ug01au074k511u78le.jpg\n",
      "  - cl5lg05uj01d6074k93foas54.jpg\n",
      "\n",
      "Duplicate Group 2 (2 identical files):\n",
      "  - cl5lg05un01ia074kf2a4ayes.jpg\n",
      "  - cl5lg05uk01fu074kfuqths55.jpg\n",
      "\n",
      "Duplicate Group 3 (2 identical files):\n",
      "  - cl5lg05uf01a6074k9wdlgqlf.jpg\n",
      "  - cl5lg05ue019i074kfgtu5ot7.jpg\n",
      "\n",
      "Duplicate Group 4 (2 identical files):\n",
      "  - cl5lg05uh01c2074kaiage5hg.jpg\n",
      "  - cl5lg05uh01bu074k41u2cc4k.jpg\n",
      "\n",
      "Duplicate Group 5 (2 identical files):\n",
      "  - cl5lg05uf019q074k77b49iib.jpg\n",
      "  - cl5lg05um01h2074k1fmo9z46.jpg\n",
      "\n",
      "Duplicate Group 6 (2 identical files):\n",
      "  - cl5lg05uk01ey074k4s9dftjk.jpg\n",
      "  - cl5lg05uf01aa074k30lz1oq2.jpg\n",
      "\n",
      "Duplicate Group 7 (2 identical files):\n",
      "  - cl5lg05uk01f2074k0wkaaelh.jpg\n",
      "  - cl5lg05uj01dq074kg2o42anm.jpg\n",
      "\n",
      "Duplicate Group 8 (2 identical files):\n",
      "  - cl5lg05uk01fi074k69ht356y.jpg\n",
      "  - cl5lg05ue018y074kdq6yc9cj.jpg\n",
      "\n",
      "Duplicate Group 9 (2 identical files):\n",
      "  - cl5lg05uf01a2074k76uvfowo.jpg\n",
      "  - cl5lg05um01gy074kgdtmhjv2.jpg\n",
      "\n",
      "Duplicate Group 10 (2 identical files):\n",
      "  - cl5lg05uk01e2074kdog34t38.jpg\n",
      "  - cl5lg05uk01f6074kddjj0jee.jpg\n",
      "\n",
      "Duplicate Group 11 (2 identical files):\n",
      "  - cl5lg05uj01da074khtmo1ka4.jpg\n",
      "  - cl5lg05un01hy074k7rgke4k2.jpg\n",
      "\n",
      "Summary:\n",
      "  Total files: 700\n",
      "  Unique images: 689\n",
      "  Duplicate files: 11\n",
      "\n",
      "Debug - Full paths for first duplicate group:\n",
      "  C:\\Users\\victo\\Downloads\\Aariz\\Aariz\\train\\Cephalograms\\cl5lg05ug01au074k511u78le.jpg\n",
      "  C:\\Users\\victo\\Downloads\\Aariz\\Aariz\\train\\Cephalograms\\cl5lg05uj01d6074k93foas54.jpg\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "images_dir = Path(r\"C:\\Users\\victo\\Downloads\\Aariz\\Aariz\\train\\Cephalograms\")\n",
    "\n",
    "# %%\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Generate MD5 hash of a file\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# %%\n",
    "# Find all images and compute their hashes\n",
    "print(\"Scanning for duplicate images...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "# Get all image files (using set to avoid duplicates)\n",
    "image_files = set()\n",
    "for pattern in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff']:\n",
    "    image_files.update(images_dir.glob(pattern))\n",
    "\n",
    "# Convert back to list\n",
    "image_files = list(image_files)\n",
    "\n",
    "print(f\"Found {len(image_files)} image files\")\n",
    "print(\"Computing hashes...\")\n",
    "\n",
    "# Hash each file\n",
    "for img_path in image_files:\n",
    "    file_hash = get_file_hash(img_path)\n",
    "    hash_to_files[file_hash].append(img_path.name)\n",
    "\n",
    "# %%\n",
    "# Find and report duplicates\n",
    "duplicates_found = False\n",
    "duplicate_groups = []\n",
    "\n",
    "for file_hash, files in hash_to_files.items():\n",
    "    if len(files) > 1:\n",
    "        duplicates_found = True\n",
    "        duplicate_groups.append(files)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if duplicates_found:\n",
    "    print(f\"⚠️  Found {len(duplicate_groups)} groups of duplicate images:\\n\")\n",
    "    for i, group in enumerate(duplicate_groups, 1):\n",
    "        print(f\"Duplicate Group {i} ({len(group)} identical files):\")\n",
    "        for filename in group:\n",
    "            print(f\"  - {filename}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✅ No duplicate images found!\")\n",
    "\n",
    "# %%\n",
    "# Summary statistics\n",
    "total_unique = len(hash_to_files)\n",
    "total_files = len(image_files)\n",
    "total_duplicates = total_files - total_unique\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  Total files: {total_files}\")\n",
    "print(f\"  Unique images: {total_unique}\")\n",
    "print(f\"  Duplicate files: {total_duplicates}\")\n",
    "\n",
    "# %%\n",
    "# Debug: Show what's happening with a specific case\n",
    "if duplicate_groups:\n",
    "    print(\"\\nDebug - Full paths for first duplicate group:\")\n",
    "    first_group = duplicate_groups[0]\n",
    "    for img_path in image_files:\n",
    "        if img_path.name in first_group:\n",
    "            print(f\"  {img_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f374a",
   "metadata": {},
   "source": [
    "### Check 2: Uniqueness\n",
    "\n",
    "In order to check uniqueness, we will check whether there are multiple of the same image. To achieve this, we will create a hash of each file. This is similar to taking a fingerprint. Once we have the fingerprints, we can check whether we see 2 or more of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4332ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Checking TRAIN set\n",
      "==================================================\n",
      "Found 700 image files\n",
      "Computing hashes...\n",
      "\n",
      "⚠️  Found 11 groups of duplicate images:\n",
      "\n",
      "Duplicate Group 1 (2 identical files):\n",
      "  - cl5lg05ug01au074k511u78le.jpg\n",
      "  - cl5lg05uj01d6074k93foas54.jpg\n",
      "\n",
      "Duplicate Group 2 (2 identical files):\n",
      "  - cl5lg05un01ia074kf2a4ayes.jpg\n",
      "  - cl5lg05uk01fu074kfuqths55.jpg\n",
      "\n",
      "Duplicate Group 3 (2 identical files):\n",
      "  - cl5lg05uf01a6074k9wdlgqlf.jpg\n",
      "  - cl5lg05ue019i074kfgtu5ot7.jpg\n",
      "\n",
      "Duplicate Group 4 (2 identical files):\n",
      "  - cl5lg05uh01c2074kaiage5hg.jpg\n",
      "  - cl5lg05uh01bu074k41u2cc4k.jpg\n",
      "\n",
      "Duplicate Group 5 (2 identical files):\n",
      "  - cl5lg05uf019q074k77b49iib.jpg\n",
      "  - cl5lg05um01h2074k1fmo9z46.jpg\n",
      "\n",
      "Duplicate Group 6 (2 identical files):\n",
      "  - cl5lg05uk01ey074k4s9dftjk.jpg\n",
      "  - cl5lg05uf01aa074k30lz1oq2.jpg\n",
      "\n",
      "Duplicate Group 7 (2 identical files):\n",
      "  - cl5lg05uk01f2074k0wkaaelh.jpg\n",
      "  - cl5lg05uj01dq074kg2o42anm.jpg\n",
      "\n",
      "Duplicate Group 8 (2 identical files):\n",
      "  - cl5lg05uk01fi074k69ht356y.jpg\n",
      "  - cl5lg05ue018y074kdq6yc9cj.jpg\n",
      "\n",
      "Duplicate Group 9 (2 identical files):\n",
      "  - cl5lg05uf01a2074k76uvfowo.jpg\n",
      "  - cl5lg05um01gy074kgdtmhjv2.jpg\n",
      "\n",
      "Duplicate Group 10 (2 identical files):\n",
      "  - cl5lg05uk01e2074kdog34t38.jpg\n",
      "  - cl5lg05uk01f6074kddjj0jee.jpg\n",
      "\n",
      "Duplicate Group 11 (2 identical files):\n",
      "  - cl5lg05uj01da074khtmo1ka4.jpg\n",
      "  - cl5lg05un01hy074k7rgke4k2.jpg\n",
      "\n",
      "Summary for train:\n",
      "  Total files: 700\n",
      "  Unique images: 689\n",
      "  Duplicate files: 11\n",
      "\n",
      "==================================================\n",
      "Checking TEST set\n",
      "==================================================\n",
      "Found 150 image files\n",
      "Computing hashes...\n",
      "\n",
      "✅ No duplicate images found!\n",
      "\n",
      "Summary for test:\n",
      "  Total files: 150\n",
      "  Unique images: 150\n",
      "  Duplicate files: 0\n",
      "\n",
      "==================================================\n",
      "Checking VALID set\n",
      "==================================================\n",
      "Found 150 image files\n",
      "Computing hashes...\n",
      "\n",
      "✅ No duplicate images found!\n",
      "\n",
      "Summary for valid:\n",
      "  Total files: 150\n",
      "  Unique images: 150\n",
      "  Duplicate files: 0\n",
      "\n",
      "============================================================\n",
      "Results saved to: duplicate_check_results.txt\n",
      "\n",
      "============================================================\n",
      "OVERALL SUMMARY\n",
      "============================================================\n",
      "Across all 3 splits:\n",
      "  Total files: 1000\n",
      "  Total unique images: 989\n",
      "  Total duplicate files: 11\n",
      "\n",
      "Final results saved to: duplicate_check_results.txt\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# %%\n",
    "# Define base path and splits\n",
    "base_path = r\"C:\\Users\\victo\\Downloads\\Aariz\\Aariz\"\n",
    "splits = ['train', 'test', 'valid']\n",
    "\n",
    "# %%\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Generate MD5 hash of a file\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def check_duplicates_in_split(split_name, images_dir):\n",
    "    \"\"\"Check for duplicates in a single split\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Checking {split_name.upper()} set\")\n",
    "    print('='*50)\n",
    "    \n",
    "    # Get all image files (using set to avoid duplicates)\n",
    "    image_files = set()\n",
    "    for pattern in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tif', '*.tiff']:\n",
    "        image_files.update(images_dir.glob(pattern))\n",
    "    \n",
    "    image_files = list(image_files)\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(image_files)} image files\")\n",
    "    print(\"Computing hashes...\")\n",
    "    \n",
    "    # Hash each file\n",
    "    hash_to_files = defaultdict(list)\n",
    "    for img_path in image_files:\n",
    "        file_hash = get_file_hash(img_path)\n",
    "        hash_to_files[file_hash].append(img_path.name)\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicate_groups = []\n",
    "    for file_hash, files in hash_to_files.items():\n",
    "        if len(files) > 1:\n",
    "            duplicate_groups.append(files)\n",
    "    \n",
    "    # Report results\n",
    "    total_unique = len(hash_to_files)\n",
    "    total_files = len(image_files)\n",
    "    total_duplicates = total_files - total_unique\n",
    "    \n",
    "    result = {\n",
    "        'split': split_name,\n",
    "        'total_files': total_files,\n",
    "        'unique_images': total_unique,\n",
    "        'duplicate_files': total_duplicates,\n",
    "        'duplicate_groups': duplicate_groups\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    if duplicate_groups:\n",
    "        print(f\"\\n Found {len(duplicate_groups)} groups of duplicate images:\")\n",
    "        for i, group in enumerate(duplicate_groups, 1):\n",
    "            print(f\"\\nDuplicate Group {i} ({len(group)} identical files):\")\n",
    "            for filename in group:\n",
    "                print(f\"  - {filename}\")\n",
    "    else:\n",
    "        print(\"\\nNo duplicate images found!\")\n",
    "    \n",
    "    print(f\"\\nSummary for {split_name}:\")\n",
    "    print(f\"  Total files: {total_files}\")\n",
    "    print(f\"  Unique images: {total_unique}\")\n",
    "    print(f\"  Duplicate files: {total_duplicates}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# %%\n",
    "# Check all splits and save results\n",
    "results = []\n",
    "output_lines = []\n",
    "output_lines.append(f\"Duplicate Image Detection Report\")\n",
    "output_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "output_lines.append(\"=\"*60)\n",
    "\n",
    "for split in splits:\n",
    "    images_dir = Path(base_path) / split / \"Cephalograms\"\n",
    "    \n",
    "    if images_dir.exists():\n",
    "        result = check_duplicates_in_split(split, images_dir)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "            \n",
    "            # Add to output file\n",
    "            output_lines.append(f\"\\n{split.upper()} SET\")\n",
    "            output_lines.append(\"-\"*40)\n",
    "            output_lines.append(f\"Total files: {result['total_files']}\")\n",
    "            output_lines.append(f\"Unique images: {result['unique_images']}\")\n",
    "            output_lines.append(f\"Duplicate files: {result['duplicate_files']}\")\n",
    "            \n",
    "            if result['duplicate_groups']:\n",
    "                output_lines.append(f\"\\nFound {len(result['duplicate_groups'])} duplicate groups:\")\n",
    "                for i, group in enumerate(result['duplicate_groups'], 1):\n",
    "                    output_lines.append(f\"\\n  Group {i} ({len(group)} identical files):\")\n",
    "                    for filename in group:\n",
    "                        output_lines.append(f\"    - {filename}\")\n",
    "            else:\n",
    "                output_lines.append(\"\\nNo duplicates found.\")\n",
    "    else:\n",
    "        print(f\"\\nDirectory not found: {images_dir}\")\n",
    "        output_lines.append(f\"\\n{split.upper()} SET: Directory not found\")\n",
    "\n",
    "# %%\n",
    "# Save results to file\n",
    "output_file = \"duplicate_check_results.txt\"\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('\\n'.join(output_lines))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# %%\n",
    "# Overall summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print('='*60)\n",
    "\n",
    "total_all = sum(r['total_files'] for r in results)\n",
    "unique_all = sum(r['unique_images'] for r in results)\n",
    "duplicates_all = sum(r['duplicate_files'] for r in results)\n",
    "\n",
    "print(f\"Across all {len(results)} splits:\")\n",
    "print(f\"  Total files: {total_all}\")\n",
    "print(f\"  Total unique images: {unique_all}\")\n",
    "print(f\"  Total duplicate files: {duplicates_all}\")\n",
    "\n",
    "# Add overall summary to file\n",
    "output_lines.append(f\"\\n{'='*60}\")\n",
    "output_lines.append(\"OVERALL SUMMARY\")\n",
    "output_lines.append(f\"Total files across all splits: {total_all}\")\n",
    "output_lines.append(f\"Total unique images: {unique_all}\")\n",
    "output_lines.append(f\"Total duplicate files: {duplicates_all}\")\n",
    "\n",
    "# Re-save with overall summary\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write('\\n'.join(output_lines))\n",
    "\n",
    "print(f\"\\nFinal results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8262da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102 annotation files\n",
      "Found 102 image files\n",
      "\n",
      "Checking: 1.json\n",
      "Ceph ID: ceph_doc1_img1_20250924115919\n",
      "Matched image: 1.bmp\n",
      "\n",
      "Image resolution: 2089 x 1937\n",
      "\n",
      "Checking 19 landmarks:\n",
      "----------------------------------------\n",
      "✓ Sella (S): (1175, 774)\n",
      "✓ Nasion (N): (1632, 639)\n",
      "✓ Orbitale (Or): (1597, 862)\n",
      "✓ Porion (Po): (1022, 932)\n",
      "✓ Anterior Nasal Spine (ANS): (1728, 1129)\n",
      "✓ Posterior Nasal Spine (PNS): (1697, 1474)\n",
      "✓ A-point (A): (1686, 1582)\n",
      "✓ B-point (B): (1638, 1630)\n",
      "✓ Pogonion (Pog): (1671, 1621)\n",
      "✓ Menton (Me): (1177, 1367)\n",
      "✓ Gnathion (Gn): (1791, 1345)\n",
      "✓ Gonion (Go): (1800, 1330)\n",
      "✓ Articulare (Ar): (1894, 1221)\n",
      "✓ Lower Incisor Tip (LIT): (1872, 1401)\n",
      "✓ Upper Incisor Tip (UIT): (1834, 1131)\n",
      "✓ Soft Tissue Pogonion (Pos): (1773, 1598)\n",
      "✓ Subnasale (Sn): (1351, 1111)\n",
      "✓ Labrale superius (Ls): (1750, 1094)\n",
      "✓ Labrale inferius (Li): (1103, 1054)\n",
      "----------------------------------------\n",
      "\n",
      "Result: 0 invalid landmarks out of 19\n",
      "\n",
      "Validating all annotation files...\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "LANDMARK VALIDATION SUMMARY\n",
      "Files with invalid landmarks: 0/102\n",
      "Total invalid landmarks: 0\n",
      "\n",
      "==================================================\n",
      "DUPLICATE IMAGE CHECK\n",
      "==================================================\n",
      "Found 102 image files\n",
      "Computing hashes...\n",
      "\n",
      "✅ No duplicate images found!\n",
      "Summary:\n",
      "  Total files: 102\n",
      "  Unique images: 102\n",
      "  Duplicate files: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'files_with_invalid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 215\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll landmarks are within image bounds.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFiles with invalid landmarks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfiles_with_invalid\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(annotation_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal invalid landmarks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_invalid_landmarks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Duplicate check results\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files_with_invalid' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "# %%\n",
    "# Set paths for dataset 2\n",
    "annotations_dir = Path(r\"C:\\Users\\victo\\Downloads\\dental-cepha-dataset\\dental-cepha-dataset_json\\doctor1\")\n",
    "images_dir = Path(r\"C:\\Users\\victo\\Downloads\\dental-cepha-dataset\\dental-cepha-dataset_json\\image\")\n",
    "\n",
    "# %%\n",
    "# Get first annotation and corresponding image for validation check\n",
    "annotation_files = list(annotations_dir.glob(\"*.json\"))\n",
    "image_files = list(images_dir.glob(\"*.*\"))\n",
    "\n",
    "print(f\"Found {len(annotation_files)} annotation files\")\n",
    "print(f\"Found {len(image_files)} image files\")\n",
    "\n",
    "# %%\n",
    "# Validate landmarks for first file as example\n",
    "if annotation_files and image_files:\n",
    "    annotation_file = annotation_files[0]\n",
    "    \n",
    "    # Load annotation to get ceph_id\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        ceph_id = data.get(\"ceph_id\", \"unknown\")\n",
    "    \n",
    "    print(f\"\\nChecking: {annotation_file.name}\")\n",
    "    print(f\"Ceph ID: {ceph_id}\")\n",
    "    \n",
    "    # Find matching image (try to match by ID in filename)\n",
    "    image_file = None\n",
    "    for img in image_files:\n",
    "        if ceph_id in img.stem or img.stem in ceph_id:\n",
    "            image_file = img\n",
    "            break\n",
    "    \n",
    "    if not image_file:\n",
    "        image_file = image_files[0]  # Use first image as fallback\n",
    "        print(f\"Using image: {image_file.name} (no ID match found)\")\n",
    "    else:\n",
    "        print(f\"Matched image: {image_file.name}\")\n",
    "    \n",
    "    # Get image resolution\n",
    "    img = Image.open(image_file)\n",
    "    width, height = img.size\n",
    "    print(f\"\\nImage resolution: {width} x {height}\")\n",
    "    \n",
    "    # Check each landmark\n",
    "    print(f\"\\nChecking {len(data['landmarks'])} landmarks:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    invalid_count = 0\n",
    "    for landmark in data['landmarks']:\n",
    "        name = landmark['title']\n",
    "        symbol = landmark['symbol']\n",
    "        x = landmark['value']['x']\n",
    "        y = landmark['value']['y']\n",
    "        \n",
    "        # Check if within bounds\n",
    "        if not (0 <= x < width and 0 <= y < height):\n",
    "            print(f\"❌ {name} ({symbol}): ({x}, {y}) - OUT OF BOUNDS\")\n",
    "            invalid_count += 1\n",
    "        else:\n",
    "            print(f\"✓ {name} ({symbol}): ({x}, {y})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nResult: {invalid_count} invalid landmarks out of {len(data['landmarks'])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Check All Files for Invalid Landmarks\n",
    "\n",
    "# %%\n",
    "def validate_all_landmarks():\n",
    "    \"\"\"Check all annotation files for out-of-bounds landmarks\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    files_with_invalid = 0\n",
    "    total_invalid_landmarks = 0\n",
    "    \n",
    "    print(\"\\nValidating all annotation files...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for ann_file in annotation_files:\n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "            ceph_id = data.get(\"ceph_id\", \"unknown\")\n",
    "        \n",
    "        # Find matching image\n",
    "        img_file = None\n",
    "        for img in image_files:\n",
    "            if ceph_id in img.stem or img.stem in ceph_id:\n",
    "                img_file = img\n",
    "                break\n",
    "        \n",
    "        if not img_file:\n",
    "            img_file = image_files[0] if image_files else None\n",
    "        \n",
    "        if img_file:\n",
    "            img = Image.open(img_file)\n",
    "            width, height = img.size\n",
    "            \n",
    "            invalid_in_file = 0\n",
    "            for landmark in data['landmarks']:\n",
    "                x = landmark['value']['x']\n",
    "                y = landmark['value']['y']\n",
    "                if not (0 <= x < width and 0 <= y < height):\n",
    "                    invalid_in_file += 1\n",
    "            \n",
    "            if invalid_in_file > 0:\n",
    "                files_with_invalid += 1\n",
    "                total_invalid_landmarks += invalid_in_file\n",
    "                print(f\"⚠️  {ann_file.name}: {invalid_in_file} invalid landmarks\")\n",
    "                results.append(f\"{ann_file.name}: {invalid_in_file} invalid landmarks\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"LANDMARK VALIDATION SUMMARY\")\n",
    "    print(f\"Files with invalid landmarks: {files_with_invalid}/{len(annotation_files)}\")\n",
    "    print(f\"Total invalid landmarks: {total_invalid_landmarks}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "validation_results = validate_all_landmarks()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Duplicate Image Detection\n",
    "\n",
    "# %%\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Generate MD5 hash of a file\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# %%\n",
    "# Check for duplicate images\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DUPLICATE IMAGE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "# Get unique image files\n",
    "image_files_set = set(images_dir.glob(\"*.*\"))\n",
    "image_files_list = list(image_files_set)\n",
    "\n",
    "print(f\"Found {len(image_files_list)} image files\")\n",
    "print(\"Computing hashes...\")\n",
    "\n",
    "# Hash each file\n",
    "for img_path in image_files_list:\n",
    "    file_hash = get_file_hash(img_path)\n",
    "    hash_to_files[file_hash].append(img_path.name)\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_groups = []\n",
    "for file_hash, files in hash_to_files.items():\n",
    "    if len(files) > 1:\n",
    "        duplicate_groups.append(files)\n",
    "\n",
    "# Report results\n",
    "if duplicate_groups:\n",
    "    print(f\"\\n⚠️  Found {len(duplicate_groups)} groups of duplicate images:\\n\")\n",
    "    for i, group in enumerate(duplicate_groups, 1):\n",
    "        print(f\"Duplicate Group {i} ({len(group)} identical files):\")\n",
    "        for filename in group:\n",
    "            print(f\"  - {filename}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n✅ No duplicate images found!\")\n",
    "\n",
    "# Summary\n",
    "total_unique = len(hash_to_files)\n",
    "total_files = len(image_files_list)\n",
    "total_duplicates = total_files - total_unique\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  Total files: {total_files}\")\n",
    "print(f\"  Unique images: {total_unique}\")\n",
    "print(f\"  Duplicate files: {total_duplicates}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Save Results to File\n",
    "\n",
    "# %%\n",
    "# Save all results to a text file\n",
    "output_file = \"dental_cepha_validation_results.txt\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"Dental Cepha Dataset Validation Report\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Dataset info\n",
    "    f.write(\"DATASET INFORMATION\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Annotations directory: {annotations_dir}\\n\")\n",
    "    f.write(f\"Images directory: {images_dir}\\n\")\n",
    "    f.write(f\"Total annotation files: {len(annotation_files)}\\n\")\n",
    "    f.write(f\"Total image files: {len(image_files)}\\n\\n\")\n",
    "    \n",
    "    # Landmark validation results\n",
    "    f.write(\"LANDMARK VALIDATION RESULTS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    if validation_results:\n",
    "        for result in validation_results:\n",
    "            f.write(f\"{result}\\n\")\n",
    "    else:\n",
    "        f.write(\"All landmarks are within image bounds.\\n\")\n",
    "    f.write(f\"\\nFiles with invalid landmarks: {files_with_invalid}/{len(annotation_files)}\\n\")\n",
    "    f.write(f\"Total invalid landmarks: {total_invalid_landmarks}\\n\\n\")\n",
    "    \n",
    "    # Duplicate check results\n",
    "    f.write(\"DUPLICATE IMAGE CHECK RESULTS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Total files: {total_files}\\n\")\n",
    "    f.write(f\"Unique images: {total_unique}\\n\")\n",
    "    f.write(f\"Duplicate files: {total_duplicates}\\n\\n\")\n",
    "    \n",
    "    if duplicate_groups:\n",
    "        f.write(f\"Found {len(duplicate_groups)} duplicate groups:\\n\\n\")\n",
    "        for i, group in enumerate(duplicate_groups, 1):\n",
    "            f.write(f\"Group {i} ({len(group)} identical files):\\n\")\n",
    "            for filename in group:\n",
    "                f.write(f\"  - {filename}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        f.write(\"No duplicate images found.\\n\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9d0652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 annotation files\n",
      "Found 400 image files\n",
      "\n",
      "Checking: 001.json\n",
      "Ceph ID: 001\n",
      "Matched image: 001.jpg\n",
      "\n",
      "Image resolution: 1935 x 2400\n",
      "\n",
      "Checking 19 landmarks:\n",
      "----------------------------------------\n",
      "✓ Sella (S): (835, 996)\n",
      "✓ Nasion (N): (1473, 1029)\n",
      "✓ Orbitale (Or): (1289, 1279)\n",
      "✓ Porion (Po): (604, 1228)\n",
      "✓ A-point (A): (1375, 1654)\n",
      "✓ B-point (B): (1386, 2019)\n",
      "✓ Pogonion (Pog): (1333, 2200)\n",
      "✓ Menton (Me): (1263, 2272)\n",
      "✓ Gnathion (Gn): (1305, 2252)\n",
      "✓ Gonion (Go): (694, 1805)\n",
      "✓ Lower Incisor Tip (LIT): (1460, 1870)\n",
      "✓ Upper Incisor Tip (UIT): (1450, 1864)\n",
      "✓ Labrale superius (Ls): (1588, 1753)\n",
      "✓ Labrale inferius (Li): (1569, 2013)\n",
      "✓ Subnasale (Sn): (1514, 1620)\n",
      "✓ Soft Tissue Pogonion (Pos): (1382, 2310)\n",
      "✓ Posterior Nasal Spine (PNS): (944, 1506)\n",
      "✓ Anterior Nasal Spine (ANS): (1436, 1569)\n",
      "✓ Articulare (Ar): (664, 1340)\n",
      "----------------------------------------\n",
      "\n",
      "Result: 0 invalid landmarks out of 19\n",
      "\n",
      "Validating all annotation files...\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "LANDMARK VALIDATION SUMMARY\n",
      "Files with invalid landmarks: 0/400\n",
      "Total invalid landmarks: 0\n",
      "\n",
      "==================================================\n",
      "DUPLICATE IMAGE CHECK\n",
      "==================================================\n",
      "Found 400 image files\n",
      "Computing hashes...\n",
      "\n",
      "⚠️  Found 2 groups of duplicate images:\n",
      "\n",
      "Duplicate Group 1 (2 identical files):\n",
      "  - 144.jpg\n",
      "  - 173.jpg\n",
      "\n",
      "Duplicate Group 2 (2 identical files):\n",
      "  - 038.jpg\n",
      "  - 283.jpg\n",
      "\n",
      "Summary:\n",
      "  Total files: 400\n",
      "  Unique images: 398\n",
      "  Duplicate files: 2\n",
      "\n",
      "==================================================\n",
      "Results saved to: kaggle_dataset_validation_results.txt\n",
      "\n",
      "============================================================\n",
      "ALL DATASETS VALIDATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Dataset 1: Aariz Dataset - Check 'duplicate_check_results.txt'\n",
      "Dataset 2: Dental Cepha Dataset - Check 'dental_cepha_validation_results.txt'\n",
      "Dataset 3: Kaggle Dataset - Check 'kaggle_dataset_validation_results.txt'\n",
      "\n",
      "All validation reports have been saved to text files.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Kaggle Dataset Validation\n",
    "# Dataset 3/3: Checking landmark coordinates and duplicate images\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Landmark Coordinate Validation\n",
    "\n",
    "# %%\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "\n",
    "# %%\n",
    "# Set paths for dataset 3 (Kaggle)\n",
    "annotations_dir = Path(r\"C:\\Users\\victo\\Downloads\\OneDrive_2025-10-02\\Dataset Kaggle\\Annotations\")\n",
    "images_dir = Path(r\"C:\\Users\\victo\\Downloads\\OneDrive_2025-10-02\\Dataset Kaggle\\Cephalograms\")\n",
    "\n",
    "# %%\n",
    "# Get annotation and image files\n",
    "annotation_files = list(annotations_dir.glob(\"*.json\"))\n",
    "image_files = list(images_dir.glob(\"*.*\"))\n",
    "\n",
    "print(f\"Found {len(annotation_files)} annotation files\")\n",
    "print(f\"Found {len(image_files)} image files\")\n",
    "\n",
    "# %%\n",
    "# Validate landmarks for first file as example\n",
    "if annotation_files and image_files:\n",
    "    annotation_file = annotation_files[0]\n",
    "    \n",
    "    # Load annotation to get ceph_id\n",
    "    with open(annotation_file) as f:\n",
    "        data = json.load(f)\n",
    "        ceph_id = data.get(\"ceph_id\", \"unknown\")\n",
    "    \n",
    "    print(f\"\\nChecking: {annotation_file.name}\")\n",
    "    print(f\"Ceph ID: {ceph_id}\")\n",
    "    \n",
    "    # Find matching image (try to match by ID in filename)\n",
    "    image_file = None\n",
    "    for img in image_files:\n",
    "        if ceph_id in img.stem or img.stem in ceph_id:\n",
    "            image_file = img\n",
    "            break\n",
    "    \n",
    "    if not image_file:\n",
    "        image_file = image_files[0]  # Use first image as fallback\n",
    "        print(f\"Using image: {image_file.name} (no ID match found)\")\n",
    "    else:\n",
    "        print(f\"Matched image: {image_file.name}\")\n",
    "    \n",
    "    # Get image resolution\n",
    "    img = Image.open(image_file)\n",
    "    width, height = img.size\n",
    "    print(f\"\\nImage resolution: {width} x {height}\")\n",
    "    \n",
    "    # Check each landmark\n",
    "    print(f\"\\nChecking {len(data['landmarks'])} landmarks:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    invalid_count = 0\n",
    "    for landmark in data['landmarks']:\n",
    "        name = landmark['title']\n",
    "        symbol = landmark['symbol']\n",
    "        x = landmark['value']['x']\n",
    "        y = landmark['value']['y']\n",
    "        \n",
    "        # Check if within bounds\n",
    "        if not (0 <= x < width and 0 <= y < height):\n",
    "            print(f\"❌ {name} ({symbol}): ({x}, {y}) - OUT OF BOUNDS\")\n",
    "            invalid_count += 1\n",
    "        else:\n",
    "            print(f\"✓ {name} ({symbol}): ({x}, {y})\")\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nResult: {invalid_count} invalid landmarks out of {len(data['landmarks'])}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Check All Files for Invalid Landmarks\n",
    "\n",
    "# %%\n",
    "def validate_all_landmarks():\n",
    "    \"\"\"Check all annotation files for out-of-bounds landmarks\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    files_with_invalid = 0\n",
    "    total_invalid_landmarks = 0\n",
    "    \n",
    "    print(\"\\nValidating all annotation files...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for ann_file in annotation_files:\n",
    "        with open(ann_file) as f:\n",
    "            data = json.load(f)\n",
    "            ceph_id = data.get(\"ceph_id\", \"unknown\")\n",
    "        \n",
    "        # Find matching image\n",
    "        img_file = None\n",
    "        for img in image_files:\n",
    "            if ceph_id in img.stem or img.stem in ceph_id:\n",
    "                img_file = img\n",
    "                break\n",
    "        \n",
    "        if not img_file:\n",
    "            img_file = image_files[0] if image_files else None\n",
    "        \n",
    "        if img_file:\n",
    "            img = Image.open(img_file)\n",
    "            width, height = img.size\n",
    "            \n",
    "            invalid_in_file = 0\n",
    "            for landmark in data['landmarks']:\n",
    "                x = landmark['value']['x']\n",
    "                y = landmark['value']['y']\n",
    "                if not (0 <= x < width and 0 <= y < height):\n",
    "                    invalid_in_file += 1\n",
    "            \n",
    "            if invalid_in_file > 0:\n",
    "                files_with_invalid += 1\n",
    "                total_invalid_landmarks += invalid_in_file\n",
    "                print(f\"⚠️  {ann_file.name}: {invalid_in_file} invalid landmarks\")\n",
    "                results.append(f\"{ann_file.name}: {invalid_in_file} invalid landmarks\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"LANDMARK VALIDATION SUMMARY\")\n",
    "    print(f\"Files with invalid landmarks: {files_with_invalid}/{len(annotation_files)}\")\n",
    "    print(f\"Total invalid landmarks: {total_invalid_landmarks}\")\n",
    "    \n",
    "    return results, files_with_invalid, total_invalid_landmarks\n",
    "\n",
    "validation_results, files_with_invalid, total_invalid_landmarks = validate_all_landmarks()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Duplicate Image Detection\n",
    "\n",
    "# %%\n",
    "def get_file_hash(filepath):\n",
    "    \"\"\"Generate MD5 hash of a file\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "# %%\n",
    "# Check for duplicate images\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DUPLICATE IMAGE CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "hash_to_files = defaultdict(list)\n",
    "\n",
    "# Get unique image files\n",
    "image_files_set = set(images_dir.glob(\"*.*\"))\n",
    "image_files_list = list(image_files_set)\n",
    "\n",
    "print(f\"Found {len(image_files_list)} image files\")\n",
    "print(\"Computing hashes...\")\n",
    "\n",
    "# Hash each file\n",
    "for img_path in image_files_list:\n",
    "    file_hash = get_file_hash(img_path)\n",
    "    hash_to_files[file_hash].append(img_path.name)\n",
    "\n",
    "# Find duplicates\n",
    "duplicate_groups = []\n",
    "for file_hash, files in hash_to_files.items():\n",
    "    if len(files) > 1:\n",
    "        duplicate_groups.append(files)\n",
    "\n",
    "# Report results\n",
    "if duplicate_groups:\n",
    "    print(f\"\\n⚠️  Found {len(duplicate_groups)} groups of duplicate images:\\n\")\n",
    "    for i, group in enumerate(duplicate_groups, 1):\n",
    "        print(f\"Duplicate Group {i} ({len(group)} identical files):\")\n",
    "        for filename in group:\n",
    "            print(f\"  - {filename}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\n✅ No duplicate images found!\")\n",
    "\n",
    "# Summary\n",
    "total_unique = len(hash_to_files)\n",
    "total_files = len(image_files_list)\n",
    "total_duplicates = total_files - total_unique\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  Total files: {total_files}\")\n",
    "print(f\"  Unique images: {total_unique}\")\n",
    "print(f\"  Duplicate files: {total_duplicates}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Save Results to File\n",
    "\n",
    "# %%\n",
    "# Save all results to a text file\n",
    "output_file = \"kaggle_dataset_validation_results.txt\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"Kaggle Dataset Validation Report\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Dataset info\n",
    "    f.write(\"DATASET INFORMATION\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Annotations directory: {annotations_dir}\\n\")\n",
    "    f.write(f\"Images directory: {images_dir}\\n\")\n",
    "    f.write(f\"Total annotation files: {len(annotation_files)}\\n\")\n",
    "    f.write(f\"Total image files: {len(image_files)}\\n\\n\")\n",
    "    \n",
    "    # Landmark validation results\n",
    "    f.write(\"LANDMARK VALIDATION RESULTS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    if validation_results:\n",
    "        for result in validation_results:\n",
    "            f.write(f\"{result}\\n\")\n",
    "    else:\n",
    "        f.write(\"All landmarks are within image bounds.\\n\")\n",
    "    f.write(f\"\\nFiles with invalid landmarks: {files_with_invalid}/{len(annotation_files)}\\n\")\n",
    "    f.write(f\"Total invalid landmarks: {total_invalid_landmarks}\\n\\n\")\n",
    "    \n",
    "    # Duplicate check results\n",
    "    f.write(\"DUPLICATE IMAGE CHECK RESULTS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    f.write(f\"Total files: {total_files}\\n\")\n",
    "    f.write(f\"Unique images: {total_unique}\\n\")\n",
    "    f.write(f\"Duplicate files: {total_duplicates}\\n\\n\")\n",
    "    \n",
    "    if duplicate_groups:\n",
    "        f.write(f\"Found {len(duplicate_groups)} duplicate groups:\\n\\n\")\n",
    "        for i, group in enumerate(duplicate_groups, 1):\n",
    "            f.write(f\"Group {i} ({len(group)} identical files):\\n\")\n",
    "            for filename in group:\n",
    "                f.write(f\"  - {filename}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    else:\n",
    "        f.write(\"No duplicate images found.\\n\")\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Summary Across All Three Datasets\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL DATASETS VALIDATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDataset 1: Aariz Dataset - Check 'duplicate_check_results.txt'\")\n",
    "print(\"Dataset 2: Dental Cepha Dataset - Check 'dental_cepha_validation_results.txt'\")\n",
    "print(\"Dataset 3: Kaggle Dataset - Check 'kaggle_dataset_validation_results.txt'\")\n",
    "print(\"\\nAll validation reports have been saved to text files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ceph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
